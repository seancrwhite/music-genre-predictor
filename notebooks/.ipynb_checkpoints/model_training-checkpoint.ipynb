{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a continuation of the model_selection page in which we attempted to train several prepackaged Scikit-Learn models on our feature data. None of the models we found were much more effective than random chance at predicting our features, if they were better at all. In the end we concluded that our system must be non-linear, thus in this noetbook we will be using Keras to build a deep learning solution to this system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear vs Non-Linear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All machine learning can be considered mathematical optimization. Even neural networks, though often spoken of as \"simulations\" of human brains, can only really be considered homages to organic brains. In our last notebook we were treating our data as a linear system, which would make machine learning (relatively) simple as each input has a single, clear output for the algorithm to attempt to estimate. It would have been very nice if this was the case, as a nonlinear system could have results that are not linearly seperable and are therefore not able to be solved with the same methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks have been used to model nonlinear systems and relationships in the past. They are particularly useful in image recognition, natural language processing, and more, and since we are using the Mel-spectrum to represent our music data we may be able to use the same systems here. Ultimately, a DNN is very similar to an ANN, with the exception that the DNN has many hidden layers between the input and output layers. Historically, convolutional neural networks have been used to solve genre classification problem in both single and multi-label contexts, so we will be exploring them here in combination with recurrent neural networks which are able to analyze time sequence data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a python API for building deep learning machine learning models with strong support for both convolutional and recurrent neural networks. It was built on top of the popular Theano library and effectively acts as a wrapper for it, however newer releases also allow for integration with Google's TensorFlow library instead. We will be using it to build our DNN below. It can be run on either CPU or GPU, we will be using the CPU option due to technical and budget limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of DLGINN, the Deep Learning Genre Identification Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mysql.connector as dbc\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Activation, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to re import all of our data, so we will reuse the code from model_selection (see model_selection.ipynb for more information on the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/seancrwhite/HDD/Data/fma/data/db/features.csv'\n",
    "\n",
    "data = np.loadtxt(datapath, delimiter=',')\n",
    "\n",
    "ids = np.reshape(data[:, 0], (1, 2057000))\n",
    "data = np.reshape(data[:, 1:], (2057, 1000, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ids = []\n",
    "i = 0\n",
    "\n",
    "for s_id in ids[0]:\n",
    "    if i % 1000 == 0:\n",
    "        u_ids.append(int(s_id))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbc.connect(port=3306,\n",
    "                 user=\"root\",\n",
    "                 passwd=\"password\",\n",
    "                 db=\"SONG\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "labels = []\n",
    "idxs = []\n",
    "\n",
    "for s_id in u_ids: \n",
    "    query = \"select * from SONG.GENRES where s_id={}\".format(s_id)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    row = cursor.fetchone()\n",
    "    \n",
    "    if row is None:\n",
    "        idxs.append(u_ids.index(s_id))\n",
    "    else:\n",
    "        labels.append(row)\n",
    "\n",
    "labels = np.array(labels)\n",
    "labels = labels[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = sorted(idxs, reverse=True)\n",
    "\n",
    "for idx in idxs:\n",
    "    data = np.delete(data, idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 16)\n",
      "(1980, 1000, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1326, 1000, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=73)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin building our model, piece by piece, using the Keras Sequential object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 1000, 100, 1)      4000      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 998, 98, 64)       640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 998, 98, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 499, 49, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 497, 47, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 497, 47, 128)      512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 248, 23, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 246, 21, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 246, 21, 128)      512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 123, 10, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 121, 8, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 121, 8, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 60, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 58, 2, 64)         73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 58, 2, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 29, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1856)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 16)                29712     \n",
      "=================================================================\n",
      "Total params: 479,216\n",
      "Trainable params: 476,192\n",
      "Non-trainable params: 3,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(axis=1, input_shape=(1000, 100, 1)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='elu'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='elu'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='elu'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='elu'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='elu'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='sigmoid', name='output'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a model in hand we can now train and evaluate it on our data. We will be using Scikit-Learn's train_test_split to make this a bit easier on ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "score_train = model.evaluate(X_train, y_train)\n",
    "score_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Training Data Accuracy: {}\".format(score_train[1]))\n",
    "print(\"Test Data Accuracy: {}\".format(score_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
